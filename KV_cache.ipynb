{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be15a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import subprocess \n",
    "import pkgutil\n",
    "\n",
    "def pip_install(pkg):\n",
    "    if pkg.replace('-', '_') not in {m.name for m in pkgutil.iter_modules()}:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
    "\n",
    "for p in ['torch', 'matplotlib', 'numpy', 'transformers', 'tqdm', 'requests']:\n",
    "    try:\n",
    "        pip_install(p)\n",
    "    except Exception as e:\n",
    "        print(f'Warning: could not ensure install for {p}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098efd49",
   "metadata": {},
   "source": [
    "Input sequence -> tokenisation -> Cache Available? \n",
    "-> Yes -> Retrieve from cache -> Generate Token\n",
    "-> No -> Compute KV Pairs -> Store in Cache -> Generate Token\n",
    "\n",
    "The above code will ignore dependency versions and will install latest available releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258536f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23fd7b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# URL for the Shakespeare dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "file_path = \"input.txt\"\n",
    "\n",
    "# Only download if the file doesn't exist yet\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    response = requests.get(url)\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"File already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee50e9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n",
      "First 100 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Length of dataset in characters: {len(text)}\")\n",
    "print(f\"First 100 characters:\\n{text[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c42f3b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d54f4999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53]\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "string_to_integer = {ch:i for i,ch in enumerate(chars)}\n",
    "integer_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_integer[c] for c in s]\n",
    "decode = lambda l: ''.join([integer_to_string[i] for i in l])\n",
    "\n",
    "print(encode(\"Hello\"))\n",
    "print(decode(encode(\"Hello\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f4965",
   "metadata": {},
   "source": [
    "Self attention Recap\n",
    "\n",
    "A minimal Transformer Block stack with multi-head self-attention\n",
    "\n",
    "Shapes:\n",
    "- Input tokens ```(B, T)```\n",
    "- Embeddings ```(B, T, C)```\n",
    "- Attention per head QKV ```(B, T, head_size)```\n",
    "- Attention weights ```(B, T, T)``` with causal mask\n",
    "\n",
    "This baseline recomputes keys/values for the entire prefix at every generation step, which becomes costly as T grows.\n",
    "\n",
    "- B - **Batch size** processing B number of distinct chunks of text at once\n",
    "- T - **Sequence Length** Each chunk has T number of characters or tokens\n",
    "- C - **Vocab Size/ Channels** The total C number of unique tokens or characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477dd47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256\n",
    "embed_size = 128\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "dropout = 0.0\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.head_size = head_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        wei = q @ k.transpose(1, 2) / math.sqrt(self.head_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
