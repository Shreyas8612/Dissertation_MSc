{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a847d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b7656be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "# URL for the Shakespeare dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "file_path = \"input.txt\"\n",
    "\n",
    "# Only download if the file doesn't exist yet\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    response = requests.get(url)\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"File already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eeae9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n",
      "First 100 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Length of dataset in characters: {len(text)}\")\n",
    "print(f\"First 100 characters:\\n{text[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369c1f7a",
   "metadata": {},
   "source": [
    "There are around **1 Million** characters in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c41b5360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22628169",
   "metadata": {},
   "source": [
    "So the ```set()``` takes the set and removes all the dublicates and keeps all characters in random order each time you run it. \n",
    "For example,\n",
    "Hello - 'h','e','l','l','o' - In Memory and set removes one of the 'l'\n",
    "\n",
    "```list()``` just fixes the order to provided fix ID and this is sorted to keep it deterministic\n",
    "\n",
    "```.join()``` just joins the characters with empty space in our case.\n",
    "For example,\n",
    "'a','b','c' becomes 'abc'\n",
    "\n",
    "Which is exactly what we see in the above result with 65 characters including space, special characters, and all alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c840e7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53]\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "string_to_integer = {ch:i for i,ch in enumerate(chars)}\n",
    "integer_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_integer[c] for c in s]\n",
    "decode = lambda l: ''.join([integer_to_string[i] for i in l])\n",
    "\n",
    "print(encode(\"Hello\"))\n",
    "print(decode(encode(\"Hello\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a8d9f1",
   "metadata": {},
   "source": [
    "Just simple encoding like a - 0 b - 1 c - 2 etc as an example and then encode the characters and decode accordingly \n",
    "\n",
    "There are various kinds of encoding like subwords and tiktoken which GPT2 uses.\n",
    "For example, \n",
    "in Tiktoken instead of 65 tokens as seen in our case they have 50257 characters or tokens. So, a encoding of Hello wouldnt be of 5 integers but rather less than 5 but the integers wouldnt be between 0 and 64 rather between 0 and 50256.\n",
    "\n",
    "There is a tradeoff between vocabularies and sequence of integers. Like, Short sequence of integers is possible with larger vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "176b465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4252644a",
   "metadata": {},
   "source": [
    "Now to encode the entire shakesphere dataset using the above simple encoding method. Pytorch is being used with dtype as whole integers rathan than float. This long is used for picking up locations from these indices. \n",
    "\n",
    "Tensor of [10, 20, 30] would be ([10, 20, 30]) like in our case shown above which is a very long sequence of characters in integers form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21847a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data)) # 90% of the data is for training and 10% for validation to generalise\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186e0dd",
   "metadata": {},
   "source": [
    "Now we cannot put the entire dataset into the transformer and train directly that would be computationally very expensive. Therefore, When training the transformer, Only chunks of the dataset is put through the transformer.\n",
    "\n",
    "So random little chunks of the training dataset is sent for training into the transformer. The max size of these randomly chosen chunks is what we **Block Size** or **Context Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08c0d8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad4c9b",
   "metadata": {},
   "source": [
    "So say a blocksize of 8 is chosen then, from the training dataset say the first 8+1 characters are taken.\n",
    "\n",
    "This has multiple examples packed into it, as these characters follow each other.\n",
    "For example,\n",
    "for the context of 18 -> 47 comes next , in the context of 18 , 47 -> 56 comes next and so on see the next code on whats being said"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77376c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target is: 47\n",
      "When input is tensor([18, 47]) the target is: 56\n",
      "When input is tensor([18, 47, 56]) the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1] # The block_size characters which is offset by 1\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd12d6",
   "metadata": {},
   "source": [
    "When the block_size of 8 is sampled from the training set these above shown are the examples hidden in the chunk of block_size.\n",
    "\n",
    "As you can see the training is being done from context as small as 1 to context as large as block size. This is usefull in inference when the model can start the sample generation with as little as 1 character context. Then the transformer will be able to predict from context of 1 to context of blocksize. \n",
    "After blocksize the transformer truncates and starts so transformer will never receive more than blocksize of inputs to predict the next character.\n",
    "\n",
    "Also the GPU must be used at full efficiency rather than having them sitting quietly, Since the GPU are great at parallel processing. Minibatches of multiple chunks of text which are stacked up in a single tensor. Each of these chunks are processed completly independently. This is what we call **Batch_dimension**\n",
    "\n",
    "- Block Size (T): The \"Context.\" How many characters into the past the model sees (e.g., 8 characters).\n",
    "\n",
    "- Batch Size (B): The \"Stack.\" How many of these 8-character chunks we feed the GPU at once.\n",
    "\n",
    "```# A single tensor containing 4 independent chunks of Shakespeare```\n",
    "\n",
    "```tensor([```\n",
    "\n",
    "```[18, 47, 56, 57, 58,  1, 15, 47],  # Chunk 1 (Random spot in the book)```\n",
    "\n",
    "```[ 1, 40, 43, 40, 43, 22,  0, 12],  # Chunk 2 (Another random spot)```\n",
    "\n",
    "```[56, 12,  1, 56, 12,  5,  0, 10],  # Chunk 3 (Another random spot)```\n",
    "\n",
    "```[12,  0,  5,  0, 12, 56, 43, 58]   # Chunk 4 (Another random spot)])```\n",
    "\n",
    "The GPU processes all 4 rows simultaneously. It calculates the loss for Row 1, Row 2, Row 3, and Row 4 at the same time, averages them, and then does one backward pass.\n",
    "\n",
    "If you set your batch size too high, your GPU runs out of memory (OOM error) because it can't \"hold\" that many chunks at once. If you set it too low, your powerful GPU sits idle waiting for work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "929ebd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "--------\n",
      "When input is [24] the target is: 43\n",
      "When input is [24, 43] the target is: 58\n",
      "When input is [24, 43, 58] the target is: 5\n",
      "When input is [24, 43, 58, 5] the target is: 57\n",
      "When input is [24, 43, 58, 5, 57] the target is: 1\n",
      "When input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
      "When input is [44] the target is: 53\n",
      "When input is [44, 53] the target is: 56\n",
      "When input is [44, 53, 56] the target is: 1\n",
      "When input is [44, 53, 56, 1] the target is: 58\n",
      "When input is [44, 53, 56, 1, 58] the target is: 46\n",
      "When input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
      "When input is [52] the target is: 58\n",
      "When input is [52, 58] the target is: 1\n",
      "When input is [52, 58, 1] the target is: 58\n",
      "When input is [52, 58, 1, 58] the target is: 46\n",
      "When input is [52, 58, 1, 58, 46] the target is: 39\n",
      "When input is [52, 58, 1, 58, 46, 39] the target is: 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58] the target is: 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1] the target is: 46\n",
      "When input is [25] the target is: 17\n",
      "When input is [25, 17] the target is: 27\n",
      "When input is [25, 17, 27] the target is: 10\n",
      "When input is [25, 17, 27, 10] the target is: 0\n",
      "When input is [25, 17, 27, 10, 0] the target is: 21\n",
      "When input is [25, 17, 27, 10, 0, 21] the target is: 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1] the target is: 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54] the target is: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # This is how many independent sequences to be processed in parallel\n",
    "block_size = 8 # Maximum context length for prediction\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs: ')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets: ')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('--------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context.tolist()} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03de773",
   "metadata": {},
   "source": [
    "ix is the part where it grabs the random chunks of data of batch_size. Since Batch_size = 4, ix will be randomly generated 4 integers between 0 and len(data) - block_size\n",
    "\n",
    "Then when stacked it will become a 4x8 tensor as seen from ```print(xb)``` where each row is the chunk of the training set as seen in the bottom for loop you can see how the targets appear for each example of context. With this training can be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7bca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.5262, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "; FXh&rszjnzQ'ItHc3N?Wg!FBdApAxrsK'I&ek\n",
      "hCjHLHL-XdoSz?tBwcRHNHfbXbP.z&A?tsJWeCtyKSKoRMt?FFfpmJDQ zvt\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # Prediction of the next token\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            \n",
    "            targets = targets.view(B*T) # From B x T to (B*T), Can use -1 if you want Pytorch to guess what you want\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:,-1,:] # Bring out the Last column of T - the prediction\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx = idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4673a0e",
   "metadata": {},
   "source": [
    "This is interesting, so from xb we have many integers. First we create a embedding matrix for all characters using ```nn.Embedding``` and from the xb when we have any integer we go to the embedding matrix and pluck out the corresponding rows of the chosen integer from xb.\n",
    "\n",
    "Thus, we get the shape of 4x8x65\n",
    "\n",
    "This way we get logits - Unnormalised version - predict what comes next based on positional knowledge\n",
    "\n",
    "Now when we have a problem Pytorch expects the channels or dimensions to be the second dimension i.e., instead of 4 x 8 x 65 we want (4*8) x 65\n",
    "\n",
    "- B - **Batch size** processing B number of distinct chunks of text at once\n",
    "- T - **Sequence Length** Each chunk has T number of characters or tokens\n",
    "- C - **Vocab Size/ Channels** The total C number of unique tokens or characters\n",
    "\n",
    "In forward - ```nn.Embedding```Shape is (B, T) $\\to$ (4, 8) - Embedding Table with character ID in each chunk.\n",
    "Logits - The Embedding layer replaces every singel integer with a row of numbers of size C - (B,T,C) $\\to$ (4,8,65)\n",
    "\n",
    "PyTorch's cross_entropy function expects a 2D table: [List of Examples, List of Classes]. It doesn't know how to handle the 3D \"Time\" dimension in this specific context.\n",
    "so by \"flatten\" the Batch and Time dimensions together. We stop caring which batch a character came from; we just treat them all as independent predictions.\n",
    "Combine $B$ and $T$ into a single dimension.$4 \\times 8 = 32$.\n",
    "Shape $(32, 65)$.  We took the 4 separate \"sheets\" of predictions and stacked them into one long list of 32 predictions.\n",
    "\n",
    "The `generate` Method (Inference)This method creates new text, one character at a time.\n",
    "\n",
    " `logits, loss = self(idx)`* **Input `idx`:** Shape (B, T).\n",
    "* Let's say we start with just 1 character per batch: (1, 1).\n",
    "\n",
    "\n",
    "* **Output `logits`:** Shape (B, T, C) $\\rightarrow$ (1, 1, 65).\n",
    "* The model gives us a probability score for *every* character in the sequence.\n",
    "\n",
    "`logits = logits[:, -1, :]` (The Critical Step)* **The Goal:** We only care about predicting the **next** character. This depends strictly on the **last** character currently in the sequence. We don't care about the predictions for the characters that happened 5 steps agoâ€”we already know what those are.\n",
    "* **The Slicing:**\n",
    "* `:` (Keep all Batches)\n",
    "* `-1` (Take only the **last** timestep T)\n",
    "* `:` (Keep all Channels/Scores)\n",
    "\n",
    "\n",
    "* **Output:** Shape (B, C) $\\rightarrow$ (1, 65).\n",
    "* We have removed the Time dimension. We now just have the scores for \"What comes next?\"\n",
    "\n",
    " `probs = F.softmax(logits, dim=-1)`* **Operation:** Convert raw scores (logits) into percentages (probabilities).\n",
    "* **Shape:** Unchanged (B, C) $\\rightarrow$ (1, 65).\n",
    "* Example: `[0.1, 0.05, 0.8, ...]` (There is an 80% chance the next char is 'e').\n",
    "\n",
    " `idx_next = torch.multinomial(probs, ...)`* **Operation:** Roll the dice based on the probabilities to pick **1** winner.\n",
    "* **Output:** Shape (B, 1) $\\rightarrow$ (1, 1).\n",
    "* This is the integer ID of the new character (e.g., the ID for 'e').\n",
    "\n",
    "`idx = torch.cat((idx, idx_next), dim=1)`* **Operation:** Glue the new character onto the end of the existing sequence.\n",
    "* **Input:** `idx` (1, 1) and `idx_next` (1, 1).\n",
    "* **Output:** Shape (1, 2).\n",
    "* The sequence has grown! Now the loop repeats, but next time `logits` will look at this new character to predict the third one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3562e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3) # Updating the weights using Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5b63747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3125789165496826\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db03386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "m lt igo\n",
      "The the yme o blulesinotereder torll, S: enoulaimod G copicll thilid t is a Y:\n",
      "NColy D:\n",
      "Wis\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "895a7541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mathematical trick in Self-Attention\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d5d1e5",
   "metadata": {},
   "source": [
    "In terms of attention, where tokens start talking to each other. we do not want the talking to happen with the future tokens. Because, we are trying to predict the future\n",
    "For example,\n",
    "3rd token **SHOULD** talk to 2nd and 1st token and **SHOULDN'T** talk to 4th, 5th 6th, etc tokens.\n",
    "\n",
    "The easiest way to do this is to take the average of all the preceeding elements - OFC this isnt the best way but lets start easy.\n",
    "So for 3rd token, take the channels of 3rd, 2nd and 1st time step and average them up which gives the context of feature vector in context of 3rd token's history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10d22219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "xbow = torch.zeros((B, T, C)) # bow - Bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # t - elements in the past of the current token so (t, C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0) # average them all (1-D vector) and store in xbow\n",
    "\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f342b3",
   "metadata": {},
   "source": [
    "As you can see the first location is the same as its the same average. but from the next token -0.0894 is the average of 0.1808 and -0.3596 and similarly with the next tokens taking all the past tokens average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a849ab56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "-------\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "-------\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b # Simple Matrix multiplication\n",
    "print('a=')\n",
    "print(a)\n",
    "print('-------')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('-------')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c409a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "-------\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "-------\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3)) # Returns a lower triangular matrix of 1's\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('-------')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('-------')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edcf53c",
   "metadata": {},
   "source": [
    "As you can see with the lower triangular matrix multiplication, the future numbers are ignored!!\n",
    "For example, \n",
    "[1 1 0] . [2 6 6] = [8] so, for the 2nd token of 6 we are only taking the past which is 2 and ignoring the future 6 (3rd element)\n",
    "\n",
    "now with simple normalisation we can take the average as well. i.e., take the element and divide by their sum of all the elements in the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97dcb845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "-------\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "-------\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('-------')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('-------')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd90e84",
   "metadata": {},
   "source": [
    "5.5 is the average of 7 and 4 just as expected getting the average of past elements. Time to use this in our weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7df917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference: 3.236345946788788e-08\n"
     ]
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "\n",
    "xbow2 = weights @ x # (T, T) @ (B, T, C) --> Since dimensions are not matching Pytorch will make it (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "diff = (xbow - xbow2).abs().max()\n",
    "print(f\"Max difference: {diff.item()}\") # To check if the are identical as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dda646f",
   "metadata": {},
   "source": [
    "This is just simialr to our matrix multiplication of c = a @ b, in our case weights is a where we take the average of all previous tokens and b is our embedding matrix. This way we can make the tokens talk to each other (Self Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f84d9d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df28a5",
   "metadata": {},
   "source": [
    "```weights.masked_fill(tril == 0, float('-inf'))```Basically for all the elements where tril = 0 place a -inf in the zeros tensor of weights.\n",
    "Why do we do this?\n",
    "When we apply softmax, we end up with exactly the same weights as previously which is a exponential normalisation for each row ```dim=-1```\n",
    "\n",
    "These weights will help us build the self attention block where the tokens can start talking to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b22c003d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "weights = F.softmax(weights, dim=-1)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e0dbac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference: 3.236345946788788e-08\n"
     ]
    }
   ],
   "source": [
    "weights = F.softmax(weights, dim=-1)\n",
    "xbow3 = weights @ x\n",
    "diff = (xbow - xbow3).abs().max()\n",
    "print(f\"Max difference: {diff.item()}\") # To check if the are identical as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a30a0b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d9de52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "print(wei)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1b130",
   "metadata": {},
   "source": [
    "Basically, Similar to what we did previously we try to take the average of all the previous tokens and the current token, by using the lower triangular approach with normalisation and softmax. In this approach we do not have data dependency of previous tokens. i.e., I want the tokens to talk to each other.\n",
    "\n",
    "This is where Self-attention comes into play, where every single token at each poisition will emit **Two vectors** - \n",
    "- **Query** - What am I looking for?\n",
    "- **Key** - What do I contain?\n",
    "\n",
    "The dot products between these i.e., dot product of query with all keys of previous tokens will provides us the weights!! So if they interact with a very high amount and learn a lot more from that perticular token compared to any other previous token!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6308cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Let us try with only ONE attention head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # (B, T, head_size)\n",
    "q = query(x)  # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)  # Last two dimensions to be transposed so -> (B, T, 16) @ (B, 16, T) = (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # To not allow the future tokens to communicate and knwo the answer before the prediction\n",
    "wei = F.softmax(wei, dim=-1)  # Normalise\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b45272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(wei[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0fb5b0",
   "metadata": {},
   "source": [
    "As you can see before the weights were a constant it was applied the same way to all the batch elements but now you can see the weights are different as all batch elements have different tokens at different positions. This is clearly visible when compared to the above wei when we started from all zeros and having all uniform distribution irrespective of what token and positon it has.\n",
    "\n",
    "Since these weights are not uniform and the last token knows what content it has and what positon it is in. \n",
    "Now it will emit a query saying for example - \"Yo!! I am a vowel I am looking for consonants before me\" \n",
    "then the keys from all the previous tokens will answer to this saying \"YO!! I am the man you are looking for!!\"\n",
    "This will create a high value like for example you can see \n",
    "- [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]\n",
    "- The 4th and 7th position has a high weight showing a strong **DOT** product between the query of the 8th position and its keys!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74861c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Let us try with only ONE attention head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # (B, T, head_size)\n",
    "q = query(x)  # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)  # Last two dimensions to be transposed so -> (B, T, 16) @ (B, 16, T) = (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # To not allow the future tokens to communicate and knwo the answer before the prediction\n",
    "wei = F.softmax(wei, dim=-1)  # Normalise\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55e746d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(wei[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945979b",
   "metadata": {},
   "source": [
    "As you can see, now the output size is 4, 8, **16**!!! as that is the head_size\n",
    "\n",
    "You can think of x has a private information to a particular token. It has a certain identity and its information is kept in vector x.\n",
    "For the purpose of a single head -\n",
    "- This is what I am interested in \n",
    "- This is what I have \n",
    "- If you find me interesting This is what I will communicate to you\n",
    "\n",
    "This is what is stored in Value which is getting aggregated in a single head between the different nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c199392",
   "metadata": {},
   "source": [
    "Attention in a communication mechanism where we have number of nodes in a directed graph of communication.\n",
    "Every node has a vector of information and it has a weighted aggregation to the nodes that point to it in a data dependent manner. Basically aggregating to the ones that are important from all the previous nodes upto them.\n",
    "\n",
    "There is no notion of space. This is exactly why we add postional encoding to the tokens\n",
    "\n",
    "Each example across batch dimension is of course processed completely independently and never \"talk\" to each other so there are 'B' seperate pools of 'T' where the pools do not talk to each other and process parallely and talking only happens between the 'T'\n",
    "\n",
    "In our case we are doing a autoregressive form of decoding the upcoming token. This is what we call Decoder attention block. But, it need not be always the case as for axample if we want like what is the sentiment of the sentence then we would like all the tokens to talk to each other in that case we simple remove the masking adn allow all tokens to talk to each other and not only the past ones in that case we call it Encoder attention block\n",
    "\n",
    "In principal attention is lot more general, In our case we have it talking with a single set of nodes that is x if we have a separate set of nodes that we want to include then we call it cross attention\n",
    "\n",
    "From the **Attention is all you need** there is a scaling of $\\sqrt(d_k)$ which is the square root of the attention head size lets see what the difference and why this is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e32d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a4e9116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0449)\n",
      "tensor(1.0700)\n",
      "tensor(17.4690)\n",
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n"
     ]
    }
   ],
   "source": [
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e030b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3095c06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9006)\n",
      "tensor(1.0037)\n",
      "tensor(0.9957)\n",
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n"
     ]
    }
   ],
   "source": [
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1492b",
   "metadata": {},
   "source": [
    "As you can see if we do the weights naively then the varience of weights goes up to the order of head_size in our case it is 16 and we get the varience of 17.47\n",
    "\n",
    "Now after scaling the varience of weights will be close to 1. This is important as these wights are going through softmax and we want the weights to be fairly diffused. If we have a very high varience then softmax will converge to a fairly one hot vectors so we do not want a spiky performance especially in initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b32c98",
   "metadata": {},
   "source": [
    "There is residual optimisation implemented here after we implemented blocks of multihead attention (Multiple Single Head attentions) and feedforward and have a simple addition of the input before transformation and after transformation so that in backpropagation this is equally distrbuted and helpful"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
